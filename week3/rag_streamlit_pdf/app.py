
import os
import json
from pathlib import Path
from typing import List, Tuple

import streamlit as st
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from openai import OpenAI

# ---------- Config ----------
VSTORE_DIR = Path("./vectorstore")
INDEX_PATH = VSTORE_DIR / "index.faiss"
DOCS_PATH = VSTORE_DIR / "docs.json"
EMBED_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
TOP_K = 4

load_dotenv()

@st.cache_resource(show_spinner=False)
def load_embed_and_index():
    if not INDEX_PATH.exists() or not DOCS_PATH.exists():
        st.error("æ‰¾ä¸åˆ°å‘é‡åº«ï¼Œè«‹å…ˆåŸ·è¡Œ `python ingest.py` å»ºç½®ã€‚")
        st.stop()
    model = SentenceTransformer(EMBED_MODEL_NAME)
    index = faiss.read_index(str(INDEX_PATH))
    with open(DOCS_PATH, "r", encoding="utf-8") as f:
        docs = json.load(f)
    return model, index, docs

def embed_query(model, q: str) -> np.ndarray:
    vec = model.encode([q], convert_to_numpy=True, normalize_embeddings=True)
    return vec.astype(np.float32)

def search(index, docs, q_vec: np.ndarray, k: int = TOP_K) -> List[dict]:
    scores, idxs = index.search(q_vec, k)
    results = []
    for score, idx in zip(scores[0], idxs[0]):
        item = docs[int(idx)]
        item = dict(item)  # copy
        item["score"] = float(score)
        results.append(item)
    return results

def build_prompt(question: str, contexts: List[dict]) -> str:
    context_text = "\n\n---\n\n".join([
        f"[ä¾†æº: {c['source']} p.{c['page']}] \n{c['text']}" for c in contexts
    ])
    prompt = f"""
ä½ æ˜¯åš´è¬¹çš„åŠ©ç†ã€‚æ ¹æ“šä¸‹åˆ—ã€Œæä¾›çš„è³‡æ–™æ®µè½ã€å›ç­”ä½¿ç”¨è€…å•é¡Œã€‚

é™åˆ¶ï¼š
- å„ªå…ˆä»¥ä¸­æ–‡ä½œç­”ã€‚
- è‹¥ç„¡æ³•å¾è³‡æ–™ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè«‹ç›´æ¥èªªã€Œæˆ‘åœ¨æ–‡ä»¶ä¸­æ²’æœ‰è¶³å¤ è³‡è¨Šå›ç­”æ­¤é¡Œã€ã€‚
- å›ç­”æ™‚å¼•ç”¨å‡ºè™•ï¼Œä¾‹å¦‚ (ä¾†æº: æª”å p.é ç¢¼)ã€‚

æä¾›çš„è³‡æ–™æ®µè½ï¼š
{context_text}

ä½¿ç”¨è€…å•é¡Œï¼š{question}
"""
    return prompt.strip()

def generate_answer(prompt: str) -> str:
    api_key = os.environ.get("OPENAI_API_KEY", "")
    if not api_key:
        return "æœªè¨­å®š OPENAI_API_KEYï¼Œç„¡æ³•ä½¿ç”¨ OpenAI ç”Ÿæˆç­”æ¡ˆã€‚è«‹åœ¨ç’°å¢ƒè®Šæ•¸æˆ– .env è¨­å®šã€‚"
    client = OpenAI(api_key=api_key)
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role":"system","content":"ä½ æ˜¯å°ˆæ¥­æ–‡ä»¶åŠ©ç†ã€‚"},
                  {"role":"user","content":prompt}],
        temperature=0.2,
    )
    return resp.choices[0].message.content.strip()

st.set_page_config(page_title="PDF RAG Chatbot", page_icon="ğŸ“š", layout="wide")
st.title("ğŸ“š PDF RAG Chatbot")
st.caption("æŠŠ PDF è½‰æˆå‘é‡è³‡æ–™åº«ï¼ˆFAISSï¼‰ï¼Œé€éæ“·å– + LLM å›ç­”ã€‚")

with st.sidebar:
    st.header("æ­¥é©Ÿ")
    st.markdown("1. æŠŠ PDF æ”¾åˆ° `./data`")
    st.markdown("2. åŸ·è¡Œ `python ingest.py` å»ºå‘é‡åº«")
    st.markdown("3. é€™è£¡è¼¸å…¥å•é¡Œé–‹å§‹èŠå¤©")
    st.divider()
    st.markdown("**è¨­å®š**")
    TOP_K = st.slider("æ¯æ¬¡æ“·å–æ®µè½æ•¸ (Top-K)", 2, 10, 4)

model, index, docs = load_embed_and_index()

question = st.text_input("è¼¸å…¥ä½ çš„å•é¡Œï¼ˆå¯ä¸­è‹±ï¼‰", placeholder="ä¾‹å¦‚ï¼šåˆç´„è§£ç´„æµç¨‹æ˜¯ä»€éº¼ï¼Ÿ")
go = st.button("é€å‡º", type="primary")

if go and question.strip():
    with st.spinner("æœå°‹å‘é‡è³‡æ–™åº«ä¸­..."):
        q_vec = embed_query(model, question)
        hits = search(index, docs, q_vec, k=TOP_K)

    with st.expander("æª¢ç´¢åˆ°çš„æ®µè½ (Top-K)", expanded=False):
        for h in hits:
            st.markdown(f"**{h['source']} p.{h['page']} (score={h['score']:.3f})**")
            st.write(h["text"])

    prompt = build_prompt(question, hits)

    with st.spinner("LLM ç”Ÿæˆå›ç­”ä¸­..."):
        answer = generate_answer(prompt)

    st.subheader("å›ç­”")
    st.write(answer)
