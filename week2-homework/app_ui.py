import os
import json
import time
import asyncio
import streamlit as st
from dataclasses import dataclass, asdict
from dotenv import load_dotenv
from tabulate import tabulate
from openai import OpenAI

# â”€â”€â”€ è®€å–ç’°å¢ƒè®Šæ•¸ â”€â”€â”€
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()

@dataclass
class LLMAnswer:
    provider: str
    model: str
    answer: str

@dataclass
class LLMEval:
    provider: str
    model: str
    clarity: float
    argument_strength: float
    overall: float
    rationale: str

def get_openai() -> OpenAI:
    return OpenAI(api_key=OPENAI_API_KEY)

def chat(model: str, system_prompt: str, user_prompt: str,
         temperature: float = 0.3, max_tokens: int = 1200) -> str:
    client = get_openai()
    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt.strip()},
            {"role": "user", "content": user_prompt.strip()},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return (resp.choices[0].message.content or "").strip()

# â”€â”€â”€ æç¤ºè© â”€â”€â”€
QUESTION_PROMPT = """ä½ æ˜¯ä¸€ä½åš´è¬¹çš„å‡ºé¡Œå°ˆå®¶ï¼Œè«‹æå‡ºä¸€å€‹ã€Œæ—…éŠæ´»å‹•ã€çš„é«˜é›£åº¦å•é¡Œï¼Œéœ€è€ƒé©—éœ€æ±‚æ‹†è§£ã€è¡Œç¨‹è¨­è¨ˆèˆ‡é¢¨éšªå‚™æ´ã€‚åªè¼¸å‡ºé¡Œç›®æœ¬èº«ã€‚"""

ANSWER_PROMPT_TEMPLATE = """é¡Œç›®ï¼š
{question}

è«‹æå‡ºå…·é«”è¡Œç¨‹æ–¹æ¡ˆã€é¢¨éšªå‚™æ´ã€é ç®—æ‹†è§£ï¼Œä¸¦ä»¥ç¹é«”ä¸­æ–‡å›è¦†ã€‚"""

JUDGE_PROMPT_TEMPLATE = """é¡Œç›®ï¼š
{question}

å€™é¸ç­”æ¡ˆï¼š
{answer}

è«‹ä¾ä¸‹åˆ—æŒ‡æ¨™æ‰“åˆ†ï¼Œè¼¸å‡º JSONï¼š
{{
  "clarity": <0-10>,
  "argument_strength": <0-10>,
  "overall": <0-10>,
  "rationale": "ç°¡çŸ­ä¸­æ–‡èªªæ˜"
}}"""

def extract_json(text: str) -> dict:
    try:
        s, e = text.find("{"), text.rfind("}")
        return json.loads(text[s:e+1])
    except:
        return {"clarity": 0, "argument_strength": 0, "overall": 0, "rationale": f"è§£æå¤±æ•—: {text[:200]}"}

# â”€â”€â”€ Streamlit UI â”€â”€â”€
st.title("ğŸ§ª LLM å‡ºé¡Œ / ä½œç­” / è©•å¯© Demo")

# æ¨¡å‹è¨­å®š
question_setter_model = st.selectbox("å‡ºé¡Œæ¨¡å‹", ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"])
candidate_models = st.multiselect("å€™é¸æ¨¡å‹", ["gpt-4o", "gpt-4o-mini", "gpt-4.1-mini"], default=["gpt-4o","gpt-4o-mini"])
judge_model = st.selectbox("è©•å¯©æ¨¡å‹", ["gpt-4o", "gpt-4o-mini"])

# é¡Œç›®è¼¸å…¥
user_question = st.text_area("é¡Œç›®ï¼ˆç•™ç©ºå‰‡è‡ªå‹•ç”±å‡ºé¡Œæ¨¡å‹ç”¢ç”Ÿï¼‰", "")

if st.button("é–‹å§‹åŸ·è¡Œ"):
    t0 = time.time()

    # 1) å‡ºé¡Œ
    if user_question.strip():
        question = user_question.strip()
    else:
        question = chat(question_setter_model, "ä½ æ˜¯ä¸€ä½å‡ºé¡Œå°ˆå®¶", QUESTION_PROMPT)

    st.subheader("ğŸ“Œ é¡Œç›®")
    st.write(question)

    # 2) å¤šæ¨¡å‹å›ç­”
    answers = []
    for m in candidate_models:
        ans = chat(m, "ä½ æ˜¯æ—…éŠæ´»å‹•é¡§å•", ANSWER_PROMPT_TEMPLATE.format(question=question), temperature=0.6)
        answers.append(LLMAnswer(provider="openai", model=m, answer=ans))

    st.subheader("ğŸ“ å„æ¨¡å‹å›ç­”")
    for a in answers:
        with st.expander(f"{a.model}"):
            st.write(a.answer)

    # 3) è©•å¯©
    evals = []
    for a in answers:
        raw = chat(judge_model, "ä½ æ˜¯åš´è¬¹çš„è©•å¯©", JUDGE_PROMPT_TEMPLATE.format(question=question, answer=a.answer))
        data = extract_json(raw)
        evals.append(LLMEval("openai", a.model, float(data["clarity"]), float(data["argument_strength"]), float(data["overall"]), data["rationale"]))

    # 4) æ’åºçµæœ
    ranking = sorted(evals, key=lambda x: x.overall, reverse=True)
    st.subheader("ğŸ† æ’å")
    st.table([[r.model, r.clarity, r.argument_strength, r.overall, r.rationale] for r in ranking])

    st.success(f"å®Œæˆï¼è€—æ™‚ {round(time.time()-t0,2)} ç§’")
